{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as is\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import talipp as tp\n",
    "\n",
    "# pd.set_option('display.max_rows', 20)  # Set to None to display all rows\n",
    "# pd.set_option('display.max_columns', None)  # Set to None to display all columns\n",
    "# pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_preprocessing():\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    connection = sqlite3.connect('etf_data.db')\n",
    "\n",
    "    # List of table names\n",
    "    etf_list = ['spy', 'tlt', 'hyg', 'lqd', 'vnq']\n",
    "\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {etf_list[0]}\", connection)\n",
    "\n",
    "    # # Loop through the list and display data for each table\n",
    "    # for table_name in etf_list:\n",
    "    #     df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", connection)\n",
    "    #     print(f\"Data for table: {table_name}\")\n",
    "    #     # print(df)\n",
    "\n",
    "    # Close the connection\n",
    "    connection.close()\n",
    "\n",
    "    # Rename last column to close\n",
    "    df.rename(columns={df.columns[4]: 'close'}, inplace=True)\n",
    "\n",
    "    # Assuming 'date' is the name of the column containing the dates in your DataFrame 'df'\n",
    "    df = df.sort_values(by='date', ascending=True)\n",
    "\n",
    "    # Reindex the sorted DataFrame\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = db_preprocessing()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from talipp.ohlcv import OHLCV\n",
    "from talipp.indicators import AccuDist, ADX, ALMA, AO, Aroon, ATR, BB, BOP, CCI, ChaikinOsc, ChandeKrollStop, CHOP, \\\n",
    "    CoppockCurve, DEMA, DonchianChannels, DPO, EMA, EMV, ForceIndex, HMA, Ichimoku, KAMA, KeltnerChannels, KST, KVO, \\\n",
    "    MACD, MassIndex, MeanDev, OBV, PivotsHL, ROC, RSI, ParabolicSAR, SFX, SMA, SMMA, SOBV, StdDev, Stoch, StochRSI, \\\n",
    "    SuperTrend, TEMA, TRIX, TSI, TTM, UO, VTX, VWAP, VWMA, WMA\n",
    "\n",
    "def indicators(df):\n",
    "\n",
    "    close = [float(row['close']) for index,row in df.iterrows()]\n",
    "\n",
    "    # Assuming you have a DataFrame named 'df' with columns 'open', 'high', 'low', 'close', and 'volume'\n",
    "    ohlcv = [OHLCV(float(row['open']), float(row['high']), float(row['low']), float(row['close']), float(row['volume']))\n",
    "             for index, row in df.iterrows()]\n",
    "    # close\n",
    "    # ohlcv\n",
    "\n",
    "    # Define the indicators\n",
    "    bop = BOP(ohlcv)\n",
    "    sobv = SOBV(7, ohlcv)\n",
    "    stoch = Stoch(14, 3, ohlcv)\n",
    "    force_index = ForceIndex(13, ohlcv)\n",
    "    macd = MACD(12, 26, 9, close)\n",
    "\n",
    "    # Create new columns in the DataFrame\n",
    "    for i in range(5685):  # Assuming the length of the indicators matches the DataFrame length (5865 rows)\n",
    "        df.loc[i, 'BOP'] = bop[i]\n",
    "        df.loc[i, 'SOBV'] = sobv[i]\n",
    "        df.loc[i, 'Stoch'] = stoch[i].k\n",
    "        df.loc[i, 'ForceIndex'] = force_index[i]\n",
    "        df.loc[i, 'MACD'] = macd[i].histogram\n",
    "\n",
    "    return df\n",
    "\n",
    "df = indicators(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # print(f'AccuDist: {AccuDist(ohlcv)[-1]}')\n",
    "    # print(f'ADX: {ADX(14, 14, ohlcv)[-1]}')\n",
    "    # print(f'ALMA: {ALMA(9, 0.85, 6, close)[-1]}')\n",
    "    # print(f'AO: {AO(5, 34, ohlcv)[-1]}')\n",
    "    # print(f'Aroon: {Aroon(14, ohlcv)[-1]}')\n",
    "    # print(f'ATR: {ATR(14, ohlcv)[-1]}')\n",
    "    # print(f'BB: {BB(20, 2, close)[-1]}')\n",
    "    # print(f'BOP: {BOP(ohlcv)[-1]}')\n",
    "    # print(f'CCI: {CCI(20, ohlcv)[-1]}')\n",
    "    # print(f'ChaikinOsc: {ChaikinOsc(3, 10, ohlcv)[-1]}')\n",
    "    # print(f'ChandeKrollStop: {ChandeKrollStop(10, 2, 9, ohlcv)[-1]}')\n",
    "    # print(f'CHOP: {CHOP(14, ohlcv)[-1]}')\n",
    "    # print(f'CoppockCurve: {CoppockCurve(11, 14, 10, close)[-1]}')\n",
    "    # print(f'DEMA: {DEMA(20, close)[-1]}')\n",
    "    # print(f'DonchianChannels: {DonchianChannels(20, ohlcv)[-1]}')\n",
    "    # print(f'DPO: {DPO(20, close)[-1]}')\n",
    "    # print(f'EMA: {EMA(20, close)[-1]}')\n",
    "    # print(f'EMV: {EMV(14, 10000, ohlcv)[-1]}')\n",
    "    # print(f'ForceIndex: {ForceIndex(13, ohlcv)[-1]}')\n",
    "    # print(f'HMA: {HMA(9, close)[-1]}')\n",
    "    # print(f'Ichimoku: {Ichimoku(26, 9, 52, 52, 26, ohlcv)[-1]}')\n",
    "    # print(f'KAMA: {KAMA(14, 2, 30, close)[-1]}')\n",
    "    # print(f'KeltnerChannels: {KeltnerChannels(20, 26, 1, 1, ohlcv)[-1]}')\n",
    "    # print(f'KST: {KST(10, 10, 15, 10, 20, 10, 30, 15, 9, close)[-1]}')\n",
    "    # print(f'KVO: {KVO(34, 55, ohlcv)[-1]}')\n",
    "    # print(f'MACD: {MACD(12, 26, 9, close)[-1]}')\n",
    "    # print(f'MassIndex: {MassIndex(9, 9, 10, ohlcv)[-1]}')\n",
    "    # print(f'MeanDev: {MeanDev(10, close)[-1]}')\n",
    "    # print(f'OBV: {OBV(ohlcv)[-1]}')\n",
    "    # print(f'Pivots: {PivotsHL(15, 15, ohlcv)[-4:]}')\n",
    "    # print(f'ROC: {ROC(9, close)[-1]}')\n",
    "    # print(f'RSI: {RSI(14, close)[-1]}')\n",
    "    # print(f\"SAR: {ParabolicSAR(0.02, 0.02, 0.2, ohlcv)[-20:]}\")\n",
    "    # print(f'SFX: {SFX(12, 12, 3, ohlcv)[-1]}')\n",
    "    # print(f'SMA: {SMA(20, close)[-1]}')\n",
    "    # print(f'SMMA: {SMMA(7, close)[-1]}')\n",
    "    # print(f'SOBV: {SOBV(7, ohlcv)[-1]}')\n",
    "    # print(f'StdDev: {StdDev(7, close)[-1]}')\n",
    "    # print(f'Stoch: {Stoch(14, 3, ohlcv)[-1]}')\n",
    "    # print(f'StochRSI: {StochRSI(14, 14, 3, 3, close)[-1]}')\n",
    "    # print(f'SuperTrend: {SuperTrend(10, 3, ohlcv)[-20:]}')\n",
    "    # print(f'TEMA: {TEMA(20, close)[-1]}')\n",
    "    # print(f'TRIX: {TRIX(18, close)[-1]}')\n",
    "    # print(f'TSI: {TSI(13, 25, close)[-1]}')\n",
    "    # print(f'TTM: {TTM(20, input_values = ohlcv)[-20:]}')\n",
    "    # print(f'UO: {UO(7, 14, 28, ohlcv)[-1]}')\n",
    "    # print(f'VTX: {VTX(14, ohlcv)[-1]}')\n",
    "    # print(f'VWAP: {VWAP(ohlcv)[-1]}')\n",
    "    # print(f'VWMA: {VWMA(20, ohlcv)[-1]}')\n",
    "    # print(f'WMA: {WMA(9, close)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(df):\n",
    "\n",
    "    # Assuming 'close' is the name of the column containing the close prices in your DataFrame 'df'\n",
    "    # Calculate the percentage change 5 rows below the current row\n",
    "    df['percentage_change'] = df['close'].shift(-5) / df['close'] - 1\n",
    "    df['percentage_change'] = df['percentage_change'].shift(5)\n",
    "\n",
    "    # Create the new column based on the conditions\n",
    "    conditions = [\n",
    "        (df['percentage_change'] >= 0.01),\n",
    "        (df['percentage_change'] <= -0.01)\n",
    "    ]\n",
    "\n",
    "    values = [2, 0]\n",
    "\n",
    "    # Default value (in between)\n",
    "    df['label'] = np.select(conditions, values, default=1)\n",
    "\n",
    "    # Drop the 'percentage_change' column if you don't need it anymore\n",
    "    # df = df.drop(columns=['percentage_change'])\n",
    "\n",
    "    # Print the DataFrame with the new column\n",
    "    # df\n",
    "\n",
    "    def create_lagged_columns(df, column_name):\n",
    "        for i in range(1, 11):\n",
    "            new_column_name = f'{column_name}_lag_{i}'\n",
    "            df[new_column_name] = df[column_name].shift(i)\n",
    "\n",
    "    list_of_indicators = ['BOP', 'SOBV', 'Stoch', 'ForceIndex', 'MACD']\n",
    "    for indicator in list_of_indicators:\n",
    "        create_lagged_columns(df, indicator)\n",
    "\n",
    "\n",
    "    list_of_indicators = ['BOP', 'SOBV', 'Stoch', 'ForceIndex', 'MACD']\n",
    "    for indicator in list_of_indicators:\n",
    "        create_lagged_columns(df, indicator)\n",
    "\n",
    "    nan_counts = df.isna().sum()\n",
    "    # nan_counts\n",
    "\n",
    "    # Drop na\n",
    "    df = df.dropna()\n",
    "    # df\n",
    "\n",
    "    return df\n",
    "\n",
    "df = features(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (1433, 8, 4),\n",
      " y.shape: (1433,),\n",
      " y: [2 1 0 ... 1 1 2]\n",
      "y_onehot.shape: (1433, 3),\n",
      " y.shape: (1433, 1),\n",
      " y_onehot[0]: [0. 0. 1.]\n",
      "X: tensor([[0.1885, 0.3431, 0.3843, 0.6152],\n",
      "        [0.1888, 0.0685, 0.3858, 0.5828],\n",
      "        [0.1891, 0.1453, 0.4137, 0.6379],\n",
      "        [0.1887, 0.2870, 0.3655, 0.6534],\n",
      "        [0.1883, 0.0628, 0.3699, 0.6277],\n",
      "        [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "        [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "        [0.1884, 0.4017, 0.4417, 0.7280]], requires_grad=True),\n",
      " y: tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def train_test(df, stack_size=8, step_size=1):\n",
    "\n",
    "    df3 = df[['SOBV', 'Stoch', 'ForceIndex', 'MACD']]\n",
    "    df4 = df['label']\n",
    "\n",
    "    df3 = df3[:1440]\n",
    "    df3 = df3.reset_index(drop=True)\n",
    "\n",
    "    # Apply MinMaxScaler to the DataFrame\n",
    "    scaler = MinMaxScaler()\n",
    "    df3_scaled = scaler.fit_transform(df3)\n",
    "\n",
    "    # Initialize an empty list to store the new arrays\n",
    "    new_arrays = []\n",
    "\n",
    "    # Iterate over the original DataFrame to create the new arrays\n",
    "    for i in range(0, len(df3_scaled) - stack_size + 1, step_size):\n",
    "        # Get a subset of arrays to stack\n",
    "        arrays_subset = df3_scaled[i:i + stack_size, :]\n",
    "        # Append the subset to the list of new arrays\n",
    "        new_arrays.append(arrays_subset)\n",
    "\n",
    "    # Convert the list of new arrays into a new 3D array\n",
    "    df3_reshaped = np.stack(new_arrays)\n",
    "\n",
    "    X = df3_reshaped.copy()\n",
    "    y = df4[7:1440].values\n",
    "    print(f\"X.shape: {X.shape},\\n y.shape: {y.shape},\\n y: {y}\")\n",
    "    # count = 0\n",
    "    # if count <=1: \n",
    "    #     print(f\"X: {X}\")\n",
    "        # count += 1\n",
    "    # Reshape y to a 2D array (required by OneHotEncoder)\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    # Create an instance of the OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Fit and transform the data to one-hot encoded representation\n",
    "    y_onehot = encoder.fit_transform(y)\n",
    "    print(f\"y_onehot.shape: {y_onehot.shape},\\n y.shape: {y.shape},\\n y_onehot[0]: {y_onehot[0]}\")\n",
    "    return X, y_onehot\n",
    "\n",
    "class OrderbookDataset(Dataset):\n",
    "    def __init__(self, train, target):\n",
    "        self.train = train\n",
    "        self.target = target\n",
    "    def __len__(self):\n",
    "        return self.train.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        train = self.train[idx]\n",
    "        target = self.target[idx]\n",
    "        train = torch.tensor(train, dtype=torch.float32, requires_grad=True)\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        return train, target\n",
    "\n",
    "# CD(X_train, y_train).__getitem__(1)\n",
    "X, y = train_test(df)\n",
    "data = OrderbookDataset(X, y)\n",
    "X, y = data.__getitem__(1)\n",
    "print(f\"X: {X},\\n y: {y}\")\n",
    "dl = DataLoader(data, batch_size=16, shuffle=False) #, num_workers=1, pin_memory=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[0.1873, 0.4184, 0.4108, 0.6577],\n",
      "         [0.1885, 0.3431, 0.3843, 0.6152],\n",
      "         [0.1888, 0.0685, 0.3858, 0.5828],\n",
      "         [0.1891, 0.1453, 0.4137, 0.6379],\n",
      "         [0.1887, 0.2870, 0.3655, 0.6534],\n",
      "         [0.1883, 0.0628, 0.3699, 0.6277],\n",
      "         [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915]],\n",
      "\n",
      "        [[0.1885, 0.3431, 0.3843, 0.6152],\n",
      "         [0.1888, 0.0685, 0.3858, 0.5828],\n",
      "         [0.1891, 0.1453, 0.4137, 0.6379],\n",
      "         [0.1887, 0.2870, 0.3655, 0.6534],\n",
      "         [0.1883, 0.0628, 0.3699, 0.6277],\n",
      "         [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280]],\n",
      "\n",
      "        [[0.1888, 0.0685, 0.3858, 0.5828],\n",
      "         [0.1891, 0.1453, 0.4137, 0.6379],\n",
      "         [0.1887, 0.2870, 0.3655, 0.6534],\n",
      "         [0.1883, 0.0628, 0.3699, 0.6277],\n",
      "         [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212]],\n",
      "\n",
      "        [[0.1891, 0.1453, 0.4137, 0.6379],\n",
      "         [0.1887, 0.2870, 0.3655, 0.6534],\n",
      "         [0.1883, 0.0628, 0.3699, 0.6277],\n",
      "         [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486]],\n",
      "\n",
      "        [[0.1887, 0.2870, 0.3655, 0.6534],\n",
      "         [0.1883, 0.0628, 0.3699, 0.6277],\n",
      "         [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047]],\n",
      "\n",
      "        [[0.1883, 0.0628, 0.3699, 0.6277],\n",
      "         [0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345]],\n",
      "\n",
      "        [[0.1886, 0.0192, 0.4201, 0.5690],\n",
      "         [0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775]],\n",
      "\n",
      "        [[0.1888, 0.2880, 0.4340, 0.5915],\n",
      "         [0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930]],\n",
      "\n",
      "        [[0.1884, 0.4017, 0.4417, 0.7280],\n",
      "         [0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409]],\n",
      "\n",
      "        [[0.1881, 0.4987, 0.4417, 0.8212],\n",
      "         [0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623]],\n",
      "\n",
      "        [[0.1867, 0.5065, 0.4767, 0.8486],\n",
      "         [0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623],\n",
      "         [0.1807, 0.6325, 0.3925, 0.7905]],\n",
      "\n",
      "        [[0.1863, 0.7332, 0.4531, 0.9047],\n",
      "         [0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623],\n",
      "         [0.1807, 0.6325, 0.3925, 0.7905],\n",
      "         [0.1816, 0.4213, 0.4340, 0.6742]],\n",
      "\n",
      "        [[0.1848, 0.6129, 0.3920, 0.9345],\n",
      "         [0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623],\n",
      "         [0.1807, 0.6325, 0.3925, 0.7905],\n",
      "         [0.1816, 0.4213, 0.4340, 0.6742],\n",
      "         [0.1827, 0.7640, 0.6143, 0.6169]],\n",
      "\n",
      "        [[0.1823, 0.3758, 0.3964, 0.9775],\n",
      "         [0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623],\n",
      "         [0.1807, 0.6325, 0.3925, 0.7905],\n",
      "         [0.1816, 0.4213, 0.4340, 0.6742],\n",
      "         [0.1827, 0.7640, 0.6143, 0.6169],\n",
      "         [0.1834, 0.9646, 0.5996, 0.5837]],\n",
      "\n",
      "        [[0.1808, 0.3627, 0.4258, 0.9930],\n",
      "         [0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623],\n",
      "         [0.1807, 0.6325, 0.3925, 0.7905],\n",
      "         [0.1816, 0.4213, 0.4340, 0.6742],\n",
      "         [0.1827, 0.7640, 0.6143, 0.6169],\n",
      "         [0.1834, 0.9646, 0.5996, 0.5837],\n",
      "         [0.1830, 0.9204, 0.5662, 0.4441]],\n",
      "\n",
      "        [[0.1800, 0.7114, 0.4209, 0.9409],\n",
      "         [0.1799, 0.6460, 0.4056, 0.8623],\n",
      "         [0.1807, 0.6325, 0.3925, 0.7905],\n",
      "         [0.1816, 0.4213, 0.4340, 0.6742],\n",
      "         [0.1827, 0.7640, 0.6143, 0.6169],\n",
      "         [0.1834, 0.9646, 0.5996, 0.5837],\n",
      "         [0.1830, 0.9204, 0.5662, 0.4441],\n",
      "         [0.1826, 0.8640, 0.5916, 0.4178]]], grad_fn=<StackBackward0>), tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]], dtype=torch.float64)]\n",
      "torch.Size([16, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "# To check if dataloader is working\n",
    "for d in dl:\n",
    "    print(d)\n",
    "    print(d[0].shape)\n",
    "    # d[0] is your input\n",
    "    # d[1] is your label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    # Return a dictionary containing various configuration parameters for the Transformer model\n",
    "    return {\n",
    "        \"batch_size\": 8, # process 6 days at a time\n",
    "        \"num_epochs\": 20, \n",
    "        \"lr\": 10**-4, \n",
    "        \"seq_len\": 8, # max sequence length - 24 hours - 1 day\n",
    "        \"d_model\": 4, # embedding vector size if it was a word\n",
    "        \"h\": 2, # number of heads in multi-head attention \n",
    "        \"N\": 4, # number of encoder and decoder layers\n",
    "        \"dropout\": 0.1,\n",
    "        \"d_ff\": 128, # feed forward layer size\n",
    "        \"num_classes\": 3, # number of classes\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": None,\n",
    "        # \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    # Build the file path for saving the model weights for a given epoch\n",
    "    model_folder = config['model_folder']\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path('.') /model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6) -> None: # eps is for numerical stability\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # Multiplied. alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # Added. bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.float()\n",
    "        \n",
    "        # x: (Batch_size, seq_len, hidden_size)\n",
    "        # Keep the dimension from broadcasting (mean cancels the dimension when applied so dimension so flag needed)\n",
    "        mean = x.mean(-1, keepdim=True) # (Batch_size, seq_len, 1)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # eps is to prevent dividing by zero when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "        # Does not change the diemension of input tensor\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        # Create the first linear layer (fully connected) with input size d_model and output size d_ff.\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # W1 & B1 (bias is True by default)\n",
    "        \n",
    "        # Create a dropout layer with dropout probability specified by the dropout argument.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create the second linear layer (fully connected) with input size d_ff and output size d_model.\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # W2 & B2 (bias is True by default)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input tensor representing a batch of hourly market features. -> (Batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Apply the first linear layer to the input tensor. -> (Batch_size, seq_len, d_ff).\n",
    "        output_linear_1 = self.linear_1(x)\n",
    "\n",
    "        # Apply the ReLU activation function to the output of the first linear layer.\n",
    "        # The ReLU activation function returns the element-wise maximum of 0 and the input tensor.\n",
    "        # The output shape remains the same as (Batch_size, seq_len, d_ff).\n",
    "        output_relu = torch.relu(output_linear_1)\n",
    "        # try LeakyReLU, GELU, SiLU\n",
    "\n",
    "        # Apply dropout to the output of the ReLU activation function.\n",
    "        # Dropout sets a fraction of elements in the input tensor to zero randomly to improve regularization during training.\n",
    "        # The output shape remains the same as (Batch_size, seq_len, d_ff).\n",
    "        output_dropout = self.dropout(output_relu)\n",
    "\n",
    "        # Apply the second linear layer to the output of the dropout layer.\n",
    "        # The output shape is (Batch_size, seq_len, d_model), which matches the input shape.\n",
    "        output_linear_2 = self.linear_2(output_dropout)\n",
    "\n",
    "        # The final output of the FeedForwardBlock is the output of the second linear layer.\n",
    "        # print(f\"FeedForwardBlock shape: {output_linear_2.shape}\")\n",
    "        return output_linear_2\n",
    "\n",
    "class SingleHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Embedding vector size = 1\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)  # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model)  # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model)  # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=nn.Dropout):\n",
    "        d_k = query.shape[-1]  # Get the last dimension of the query tensor\n",
    "\n",
    "        # Applying the attention formula from the paper (scaled dot-product attention)\n",
    "        # (Batch_size, seq_len, d_k) -> (Batch_size, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # Apply masking if provided to handle padded tokens or future tokens\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "            # The large negative number becomes close to zero in the softmax function\n",
    "\n",
    "        # Apply the softmax activation function along the last dimension to get attention weights\n",
    "        # No change in shape: (Batch_size, seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights if dropout is provided\n",
    "        if dropout is not None:\n",
    "            # m = dropout(0.1)\n",
    "            attention_weights = dropout(attention_weights)\n",
    "\n",
    "        # Compute the weighted sum of the value tensor using the attention weights\n",
    "        # (attention_weights @ value) -> (Batch_size, seq_len, d_k) @ (Batch_size, seq_len, d_k)\n",
    "        # attention is for visualization\n",
    "        return (attention_weights @ value), attention_weights\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Apply linear transformations to get the query, key, and value tensors\n",
    "        # (Batch_size, seq_len, d_model) -> (Batch_size, seq_len, d_model)\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # Calculate the attention scores using the static method attention()\n",
    "        # x is the weighted sum of the value tensor using attention weights\n",
    "        # (Batch_size, seq_len, d_k), self.attention_scores has the attention weights\n",
    "        x, self.attention_scores = SingleHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Apply the output linear transformation and return the output tensor\n",
    "        # (Batch_size, seq_len, d_model) -> (Batch_size, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Embedding vector size\n",
    "        self.h = h  # Number of heads\n",
    "\n",
    "        # d_model must be divisible by h because we will split d_model into h heads\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"  # Check for validity\n",
    "\n",
    "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
    "\n",
    "        # Linear transformations for query, key, value, and output projections for each head\n",
    "        self.w_q = nn.Linear(d_model, d_model)  # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model)  # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model)  # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # print(f\"w_q: {self.w_q.shape}, w_k: {self.w_k.shape}, w_v: {self.w_v.shape}, w_o: {self.w_o.shape}\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=nn.Dropout):\n",
    "        d_k = query.shape[-1]  # Get the last dimension of the query tensor\n",
    "\n",
    "        # Applying the attention formula from the paper (scaled dot-product attention)\n",
    "        # (Batch_size, h, seq_len, d_k) -> (Batch_size, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # print(f\"attention_scores: {attention_scores.shape}\")\n",
    "        # order of execution:\n",
    "        # 1. transpose(-2, -1) -> (Batch_size, h, seq_len, d_k) -> (Batch_size, h, d_k, seq_len)\n",
    "        # 2. query @ key -> (Batch_size, h, seq_len, seq_len)\n",
    "        # 3. / math.sqrt(d_k) -> (Batch_size, h, seq_len, seq_len)\n",
    "\n",
    "        # Apply masking if provided to handle padded tokens or future tokens\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "            # The large negative number becomes close to zero in the softmax function\n",
    "\n",
    "        # Apply the softmax activation function along the last dimension to get attention weights\n",
    "        # No change in shape: (Batch_size, h, seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply dropout to the attention weights if dropout is provided\n",
    "        if dropout is not None:\n",
    "            # m = dropout(0.1)\n",
    "            attention_weights = dropout(attention_weights)\n",
    "\n",
    "        # Compute the weighted sum of the value tensor using the attention weights\n",
    "        # (attention_weights @ value) -> (Batch_size, h, seq_len, seq_len) @ (Batch_size, h, seq_len, d_k)\n",
    "        # attention is for visualization\n",
    "        return (attention_weights @ value), attention_weights\n",
    "        \n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # Apply linear transformations to get the query, key, and value tensors\n",
    "        # (Batch_size, seq_len, d_model) -> (Batch_size, seq_len, d_model)\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # print(f\"query: {query.shape}, key: {key.shape}, value: {value.shape}\")\n",
    "        # Split the query, key, and value into h different parts (h = Number of heads)\n",
    "        # We split d_model into h parts where each part is d_k in size\n",
    "        # (Batch_size, seq_len, d_model) -> (Batch_size, seq_len, h, d_k) -> (Batch_size, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate the attention scores using the static method attention()\n",
    "        # x is the weighted sum of the value tensor using attention weights\n",
    "        # (Batch_size, h, seq_len, d_k), self.attention_scores has the attention weights\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Reshape and combine the heads to get the output tensor\n",
    "        # .transpose() : (Batch_size, h, seq_len, d_k) -> (Batch_size, seq_len, h, d_k)\n",
    "        # .view() : (Batch_size, seq_len, h, d_k) -> (Batch_size, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Apply the output linear transformation and return the output tensor\n",
    "        # (Batch_size, seq_len, d_model) -> (Batch_size, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)    # Dropout layer with the specified dropout rate\n",
    "        self.norm = LayerNormalization()      # LayerNormalization instance\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # Apply the normalization layer to the input tensor 'x'\n",
    "        normalized_input = self.norm(x)\n",
    "        \n",
    "        # Apply the sublayer (e.g., a feedforward block or a multi-head attention block) to the normalized input\n",
    "        # The sublayer can be any neural network component that processes the input tensor\n",
    "        intermediate_output = sublayer(normalized_input)\n",
    "        \n",
    "        # In the attention is all you need paper, sublayer is applied before the normalization layer.\n",
    "        # It doesn't matter, both work fine.\n",
    "        # In fact normalizing before applying the sublayer helps with numerical stability.\n",
    "\n",
    "        # Apply dropout to the intermediate output\n",
    "        dropout_output = self.dropout(intermediate_output)\n",
    "        \n",
    "        # Add the dropout output to the original input tensor (residual connection)\n",
    "        # This is the essence of the residual connection: x + Dropout(Sublayer(LayerNormalization(x)))\n",
    "        # The addition operation allows the intermediate outputs of the sublayer to flow directly to the output\n",
    "        # and be added to the original input, bypassing the sublayer in case it does not capture useful information.\n",
    "        # This helps the model to learn residual mappings (the difference between the input and output)\n",
    "        # and facilitates training of deeper neural networks.\n",
    "        output = x + dropout_output\n",
    "\n",
    "        return output        \n",
    "        # All of the above operations can be combined into a single line as follows:\n",
    "        # return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-Head Self-Attention Block\n",
    "        self.self_attention_block = self_attention_block\n",
    "\n",
    "        # Feed-Forward Block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        # Residual Connections\n",
    "        # Create two ResidualConnection instances with the specified dropout rate\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Step 1: Self-Attention Sublayer\n",
    "        # Apply Multi-Head Self-Attention to the input 'x' with the mask 'src_mask' (encoder mask)\n",
    "        # and pass the result through a ResidualConnection.\n",
    "        # The ResidualConnection adds the input tensor 'x' to the output of the self-attention sublayer.\n",
    "        # This is the first residual connection in the encoder block.\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        # Step 2: Feed-Forward Sublayer\n",
    "        # Apply the Feed-Forward Block to the output of the self-attention sublayer and\n",
    "        # pass the result through another ResidualConnection.\n",
    "        # The ResidualConnection adds the input tensor 'x' to the output of the feed-forward sublayer.\n",
    "        # This is the second residual connection in the encoder block.\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "\n",
    "        # Return the final output after both the self-attention and feed-forward sublayers.\n",
    "        # The output tensor has shape (batch_size, seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the list of Encoder blocks (layers)\n",
    "        self.layers = layers\n",
    "\n",
    "        # Layer normalization instance to be applied after all Encoder blocks\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Iterate through the Encoder blocks (layers) and apply each block's forward pass to the input tensor 'x'\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        # Apply layer normalization to the output tensor 'x'\n",
    "        # Layer normalization is applied after processing all Encoder blocks.\n",
    "        # shape of x: (batch_size, seq_len, d_model)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.proj(x)  # Get the raw scores from the linear projection\n",
    "        # print(f\"logits shape: {logits.shape}\\n logits: {logits[0]}\")\n",
    "        # print(f\"LOGITS: {logits[0][-1]}\")\n",
    "        probabilities = torch.softmax(logits[:, -1, :], dim=-1)  # Apply softmax to get probabilities\n",
    "        # print(f\"probabilities shape: {probabilities.shape}\\nprobabilities: {probabilities}\")\n",
    "        # Apply average pooling along the second dimension (seq_len)\n",
    "        # pooled_probs = torch.mean(probabilities, dim=1)  # Shape: (16, 3)\n",
    "        # return torch.argmax(probabilities, dim=-1)  # Return the class indices with highest probabilities\n",
    "        return probabilities\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder: Encoder, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the encoder, input embeddings and projection layer\n",
    "        self.encoder = encoder\n",
    "        # self.src_embed = src_embed\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # Step 1: Source Embedding\n",
    "        # Convert the source input 'src' into embeddings using the src_embed layer.\n",
    "        # src = self.src_embed(src)\n",
    "        print(f\"src shape: {src.shape}\")\n",
    "        print(f\"src_mask shape: {src_mask.shape}\")\n",
    "        # Step 2: Encoding\n",
    "        # Pass the source embeddings through the encoder to obtain contextual representations.\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "\n",
    "        # Return the encoder output, which contains the contextual representations of the source sequence.\n",
    "        return encoder_output\n",
    "    \n",
    "    def project(self, x):\n",
    "        # Step 1: Projection\n",
    "        # Pass the input tensor 'x' through the projection_layer to perform the final classification.\n",
    "        # This maps the contextual representations to the number of classes for the classification task.\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "def build_transformer(seq_len: int = 4, d_model: int = 1, num_classes: int = 3, batch_size: int = 24, N: int = 4, h: int = 2, dropout: float = 0.1, d_ff: int = 64) -> Transformer:\n",
    "    # Create input embeddings\n",
    "    # Input data has shape (batch_size, seq_len, num_features)\n",
    "    # Batch size is the number of samples in the batch\n",
    "    # InputEmbeddings class will individually process each sample in the batch\n",
    "    # src_embed = InputEmbeddings(batch_size, seq_len, d_model)\n",
    "    # print(f\"src_embed shape: {src_embed}\")\n",
    "    encoder_blocks = []\n",
    "    # Create N Encoder blocks\n",
    "    for _ in range(N):\n",
    "        # Create the masked multi-head self-attention block and feed-forward block\n",
    "        self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "        # Combine the self-attention and feed-forward blocks to create an Encoder block\n",
    "        encoder_block = EncoderBlock(self_attention_block, feed_forward_block, dropout)\n",
    "\n",
    "        # Add the Encoder block to the list\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    # Create the Encoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, num_classes)\n",
    "\n",
    "    # Create the Transformer\n",
    "    transformer = Transformer(encoder, projection_layer)\n",
    "\n",
    "    # Initialize the weights of the transformer using Xavier uniform initialization\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # Return the built transformer\n",
    "    return transformer\n",
    "\n",
    "def causal_mask(seq_len):\n",
    "    # Create a mask that prevents the decoder from attending to future positions.\n",
    "    # The mask will have 0s above the main diagonal and 1s below it.\n",
    "    mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "def get_model(config):\n",
    "    model = build_transformer(config['seq_len'], config['d_model'], config['num_classes'], config['batch_size'], config['N'], config['h'], config['dropout'], config['d_ff'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "\n",
    "    # Make sure that weights foler exists\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # print(f\"Input df shape: {df2.shape}\")\n",
    "    # Load the datas\n",
    "    # train_dataloader, val_dataloader, test_dataloader = get_ds(config, df2)\n",
    "    \n",
    "    X, y = train_test(df)\n",
    "    # print(f\"X shape: {X.shape}, X: {X}\")\n",
    "    # print(f\"y shape: {y.shape}, y: {y}\")\n",
    "    data = OrderbookDataset(X, y)\n",
    "    train_dataloader = DataLoader(data, batch_size=16, shuffle=False) \n",
    "\n",
    "    # Load the model\n",
    "    model = get_model(config).to(device)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f\"Preloading model weights from {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        global_step = state['global_step']\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
    "        for batch in batch_iterator:\n",
    "            # print(f\"batch: {batch[0]}\")\n",
    "            encoder_input = batch[0].to(device) # (batch_size, seq_len)\n",
    "            label = batch[1].to(device)\n",
    "\n",
    "            # print(f\"encoder_input.shape: {encoder_input.shape}\")\n",
    "            # print(f\"label.shape: {label.shape}\")\n",
    "            # print(f\"label dtype: {label.dtype}\")\n",
    "\n",
    "            # Generate a causal mask\n",
    "            seq_len = encoder_input.size(1)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).to(device)\n",
    "            # print(f\"causal_mask.shape: {causal_mask.shape}\")\n",
    "\n",
    "            # Forward pass\n",
    "            encoder_output = model.encoder(encoder_input, causal_mask) # (batch_size, seq_len, d_model)\n",
    "            pred_label = model.projection_layer(encoder_output) # (batch_size, seq_len, num_classes)\n",
    "            # print(f\"pred_label.shape: {pred_label.shape}, pred_label: {pred_label}\")\n",
    "            # print(f\"label.shape: {label.shape}\", f\"label: {label}\")\n",
    "\n",
    "            # Squeeze the label tensor to remove the extra dimension\n",
    "            # Squeeze the extra dimension from the label tensor\n",
    "            # label = label.squeeze(dim=1)  # (batch_size)\n",
    "            # log_probs = label.float()\n",
    "\n",
    "            # Detach the label tensor from the computation graph\n",
    "            # with torch.inference_mode():\n",
    "                # label_detached = label.detach()\n",
    "            # label = label.float()\n",
    "\n",
    "            def cross_entropy(input, target):\n",
    "                return torch.mean(-torch.sum(target * torch.log(input), 1))\n",
    "\n",
    "\n",
    "            # Compute the loss\n",
    "            # loss = criterion(pred_label.float(), label)\n",
    "            loss = cross_entropy(pred_label.float(), label.float())\n",
    "            # print(f\"loss: {loss}, loss_grad: {loss.grad_fn}\")\n",
    "            # Compute the loss\n",
    "            # Reshape log_probs and label to align with the expected shape for NLLLoss\n",
    "            # loss = criterion(log_probs.view(-1, log_probs.size(-1)), label.view(-1))\n",
    "            batch_iterator.set_postfix({f'loss': f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss to Tensorboard\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step\n",
    "            }, model_filename\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "X.shape: (1433, 8, 4),\n",
      " y.shape: (1433,),\n",
      " y: [2 1 0 ... 1 1 2]\n",
      "y_onehot.shape: (1433, 3),\n",
      " y.shape: (1433, 1),\n",
      " y_onehot[0]: [0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|| 90/90 [00:01<00:00, 87.68it/s, loss=0.884] \n",
      "Processing epoch 01: 100%|| 90/90 [00:00<00:00, 94.06it/s, loss=0.959] \n",
      "Processing epoch 02: 100%|| 90/90 [00:00<00:00, 124.43it/s, loss=1.030]\n",
      "Processing epoch 03: 100%|| 90/90 [00:00<00:00, 110.99it/s, loss=0.996]\n",
      "Processing epoch 04: 100%|| 90/90 [00:00<00:00, 117.20it/s, loss=0.927]\n",
      "Processing epoch 05: 100%|| 90/90 [00:00<00:00, 113.39it/s, loss=0.941]\n",
      "Processing epoch 06: 100%|| 90/90 [00:00<00:00, 107.59it/s, loss=0.921]\n",
      "Processing epoch 07: 100%|| 90/90 [00:00<00:00, 121.28it/s, loss=0.982]\n",
      "Processing epoch 08: 100%|| 90/90 [00:00<00:00, 112.09it/s, loss=1.053]\n",
      "Processing epoch 09: 100%|| 90/90 [00:00<00:00, 110.46it/s, loss=0.998]\n",
      "Processing epoch 10: 100%|| 90/90 [00:00<00:00, 115.63it/s, loss=0.997]\n",
      "Processing epoch 11: 100%|| 90/90 [00:00<00:00, 109.77it/s, loss=0.988]\n",
      "Processing epoch 12: 100%|| 90/90 [00:00<00:00, 128.44it/s, loss=0.935]\n",
      "Processing epoch 13: 100%|| 90/90 [00:00<00:00, 110.54it/s, loss=0.997]\n",
      "Processing epoch 14: 100%|| 90/90 [00:00<00:00, 111.70it/s, loss=1.016]\n",
      "Processing epoch 15: 100%|| 90/90 [00:00<00:00, 106.19it/s, loss=0.948]\n",
      "Processing epoch 16: 100%|| 90/90 [00:00<00:00, 127.00it/s, loss=1.076]\n",
      "Processing epoch 17: 100%|| 90/90 [00:00<00:00, 108.92it/s, loss=1.078]\n",
      "Processing epoch 18: 100%|| 90/90 [00:00<00:00, 108.38it/s, loss=0.954]\n",
      "Processing epoch 19: 100%|| 90/90 [00:00<00:00, 106.74it/s, loss=0.944]\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3567)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(input, target):\n",
    "    return torch.mean(-torch.sum(target * torch.log(input), 1))\n",
    "\n",
    "\n",
    "y = torch.Tensor([[0, 0, 1]])\n",
    "yhat = torch.Tensor([[0.1, 0.2, 0.7]])\n",
    "cross_entropy(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop works but model does not learn. Need to tune params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below this is rough work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1437, 4, 4])\n",
      "torch.Size([1437, 1])\n"
     ]
    }
   ],
   "source": [
    "class CustDat()\n",
    "\n",
    "\n",
    "class OrderbookDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        xy = df2\n",
    "        self.x = torch.from_numpy(df2[['SOBV', 'Stoch', 'ForceIndex', 'MACD']].values) # 1440 x 4\n",
    "        self.y = torch.from_numpy(df2[['label']].values) # 1440\n",
    "        self.n_samples = xy.shape[0] # 1440\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Create the dataset\n",
    "dataset = OrderbookDataset()\n",
    "\n",
    "X, y = dataset[:]\n",
    "\n",
    "def create_rolling_windows(tensor, window_size):\n",
    "    num_windows = tensor.shape[0] - window_size + 1\n",
    "    windows = [tensor[i:i+window_size] for i in range(num_windows)]\n",
    "    return torch.stack(windows)\n",
    "\n",
    "# Define the rolling window size\n",
    "window_size = 4\n",
    "\n",
    "# Create rolling windows along the first dimension (new dimension as you mentioned)\n",
    "X_input = create_rolling_windows(X, window_size)\n",
    "y_input = y[3:]\n",
    "\n",
    "# Print the resulting tensor\n",
    "print(X_input.shape) \n",
    "print(y_input.shape) \n",
    "\n",
    "\n",
    "# Step 1: Manually split the data into training, testing, and validation sets\n",
    "# You can use slicing to achieve this. For example, splitting 70% for training, 15% for testing, and 15% for validation:\n",
    "# train_size = int(864) # 36 days\n",
    "# test_size = int(288) # 12 days\n",
    "# val_size = int(288) # 12 days\n",
    "\n",
    "# train_data = dataset[:train_size]\n",
    "# val_data = dataset[train_size:train_size+val_size]\n",
    "# test_data = dataset[train_size+val_size:train_size+val_size+test_size]\n",
    "\n",
    "\n",
    "\n",
    "# X_train, y_train = train_data\n",
    "# X_val, y_val = val_data\n",
    "# X_test, y_test = test_data\n",
    "\n",
    "# print(f\"X_train.shape: {X_train}\")\n",
    "\n",
    "# print(f\"seq_len: {seq_len}\")\n",
    "\n",
    "# Create an upper triangular mask of shape (seq_len, seq_len)\n",
    "# upper_triangular_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "# upper_triangular_mask\n",
    "# Broadcast the mask to the batch size and unsqueeze to match the encoder_mask shape\n",
    "# encoder_mask = upper_triangular_mask.unsqueeze(0).expand(encoder_mask.shape[0], -1, -1)\n",
    "\n",
    "# Now, encoder_mask will have the shape (batch_size, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "def causal_mask(seq_len):\n",
    "    mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "seq_len = 5\n",
    "mask = causal_mask(seq_len)\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Convert the data into PyTorch tensors\n",
    "# X_train = torch.tensor(df2.values, dtype=torch.float32)\n",
    "# print(f\"shape: {X_train.shape}\")\n",
    "# # Reshape X_train to have the desired shape (36 sets of 24 rows, each with 4 elements)\n",
    "# num_sets = 36\n",
    "# num_rows_per_set = 24\n",
    "# num_elements = 4\n",
    "# X_train = X_train.reshape(num_sets, num_rows_per_set, num_elements)\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df2[:1440].values  # (1440, 4) - Convert to numpy array\n",
    "# y = df2[:1440].values   # (1440,) - Convert to numpy array\n",
    "\n",
    "# # len(X)\n",
    "# # Reshape X to the desired shape [batch_dimension, seq_len=24, d_model=4]\n",
    "# num_sets = len(X) // 24\n",
    "# X = X[:num_sets * 24].reshape(num_sets, 24, 4)\n",
    "\n",
    "# print(X.shape)\n",
    "# print(X)\n",
    "\n",
    "# # Convert X and y to PyTorch tensors\n",
    "# # X = torch.tensor(X, dtype=torch.float32)\n",
    "# # y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, batch_size: int, seq_len: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input tensor of dimension (d_model, seq_len) -> (4, 24)\n",
    "\n",
    "        # Transpose the input tensor to dimension (seq_len, d_model) -> (24, 4)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Add dimension for batch dimension (batch_size, seq_len, d_model) -> (1, 24, 4)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # Repeat the tensor for batch dimension\n",
    "        # x = x.repeat(self.batch_size, 2, 1)\n",
    "\n",
    "        # Scale the tensor by square root of d_model\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "\n",
    "        # (1, 24, 4)\n",
    "        print(f\"InputEmbedding shape: {x.shape}\")\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df2\n",
    "# df2.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x[0], y[0]\n",
    "# x, y\n",
    "\n",
    "# df2.shape[0]\n",
    "# # Step 1: Reshape the 2D tensor into a 3D tensor with 60 elements along the 0th dimension\n",
    "# tensor_3d = X_train.view(60, 24, 4)\n",
    "\n",
    "# # Step 2: Create a 1D tensor for the labels\n",
    "# labels = y_train.view(60)\n",
    "\n",
    "# # Print the resulting shapes\n",
    "# print(tensor_3d.shape)  # torch.Size([60, 24, 4])\n",
    "# print(labels.shape)     # torch.Size([60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.4885e+06,  6.2780e+00, -8.8032e+06,  2.9213e-01],\n",
       "        dtype=torch.float64),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# dataiter = iter(dataloader)\n",
    "# data = dataiter.__next__()\n",
    "# features, labels = data\n",
    "# print(features, labels)\n",
    "# # Convert each row into a tensor\n",
    "# row_tensors = [torch.tensor(row, dtype=torch.float32) for _, row in df2.iterrows()]\n",
    "\n",
    "# # Concatenate all tensors along a new outer dimension (stacking them)\n",
    "# X = torch.stack(row_tensors)\n",
    "# # print(X)\n",
    "# # X[2,1]\n",
    "# # Convert the 'label' column into a tensor\n",
    "# labels = torch.tensor(df['label'].values, dtype=torch.int64)\n",
    "\n",
    "# # Print the resulting shape\n",
    "# print(X.shape)\n",
    "# print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[60, 24, 4]' is invalid for input of size 7200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Reshape the 2D tensor into a 3D tensor with 60 elements along the 0th dimension\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tensor_3d \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mview(\u001b[39m60\u001b[39;49m, \u001b[39m24\u001b[39;49m, \u001b[39m4\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Print the resulting shape\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(tensor_3d\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[60, 24, 4]' is invalid for input of size 7200"
     ]
    }
   ],
   "source": [
    "# Reshape the 2D tensor into a 3D tensor with 60 elements along the 0th dimension\n",
    "tensor_3d = X.view(60, 24, 4)\n",
    "\n",
    "# Print the resulting shape\n",
    "print(tensor_3d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        (5): TransformerBlock(\n",
    "          (attention): MultiHeadSelfAttention(\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "          )\n",
    "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (ffn): FFN(\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          )\n",
    "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (l2): Dropout(p=0.3, inplace=False)\n",
    "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
    "\n",
    "\n",
    "how did they get the out features to be 1 in the end? that is what I am looking for.\n",
    "\n",
    "what should I do after the below code to get a single output which I will apply softmax on?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InputData(nn.Module):\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, class_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a linear projection layer that maps the input 'd_model' dimensional tensor to 'class_size' dimensional tensor.\n",
    "        self.proj = nn.Linear(d_model, class_size)\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        # encoder_output has shape (batch_size, seq_len, d_model)\n",
    "        # We want to perform classification along the seq_len dimension, so we use mean pooling.\n",
    "        # We can also use max pooling or other pooling strategies.\n",
    "        pooled_output = torch.mean(encoder_output, dim=1)  # shape: (batch_size, d_model)\n",
    "        \n",
    "        # Apply the linear projection layer to the input tensor 'x', followed by a log-softmax operation along the last dimension (class_size).\n",
    "        # The log-softmax operation converts the raw scores into log-probabilities, making it more numerically stable during training.\n",
    "        # The shape changed from (batch_size, seq_len, d_model) to (batch_size, seq_len, class_size).\n",
    "        return torch.log_softmax(self.proj(encoder_output), dim=-1)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        # encoder_output has shape (batch_size, seq_len, d_model)\n",
    "        # We want to perform classification along the seq_len dimension, so we use mean pooling.\n",
    "        # We can also use max pooling or other pooling strategies.\n",
    "        pooled_output = torch.mean(encoder_output, dim=1)  # shape: (batch_size, d_model)\n",
    "\n",
    "        # Apply the linear layer for classification.\n",
    "        logits = self.linear(pooled_output)  # shape: (batch_size, num_classes)\n",
    "\n",
    "        # Apply the softmax function to get the probabilities for each class.\n",
    "        probabilities = F.softmax(logits, dim=1)  # shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits, probabilities\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, src_embed: InputData,\n",
    "                 projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the encoder, decoder, input embeddings, positional encodings, and projection layer\n",
    "        self.encoder = encoder\n",
    "        # self.decoder = decoder\n",
    "        self.src_embed = src_embed # Gets data and just adds batch dimensions. Create class for this\n",
    "        # self.tgt_embed = tgt_embed\n",
    "        # self.src_pos = src_pos\n",
    "        # self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "\n",
    "        # Step 1: Source Embedding\n",
    "        # Convert the source input 'src' into embeddings using the src_embed layer.\n",
    "        src = self.src_embed(src)\n",
    "\n",
    "        # Step 2: Encoding\n",
    "        # Pass the source embeddings through the encoder to obtain contextual representations.\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "\n",
    "        # Return the encoder output, which contains the contextual representations of the source sequence.\n",
    "        return encoder_output\n",
    "\n",
    "    # def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "    #     # Step 1: Target Embedding\n",
    "    #     # Convert the target input 'tgt' into embeddings using the tgt_embed layer.\n",
    "    #     tgt = self.tgt_embed(tgt)\n",
    "\n",
    "    #     # Step 2: Target Positional Encoding\n",
    "    #     # Apply the positional encoding to the target embeddings.\n",
    "    #     tgt = self.tgt_pos(tgt)\n",
    "\n",
    "    #     # Step 3: Decoding\n",
    "    #     # Pass the target embeddings through the decoder along with encoder_output,\n",
    "    #     # source mask 'src_mask', and target mask 'tgt_mask' to obtain decoder outputs.\n",
    "    #     decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    #     # Return the decoder output, which contains the contextual representations of the target sequence.\n",
    "    #     return decoder_output\n",
    "\n",
    "    def project(self, x):\n",
    "        # Step 1: Projection\n",
    "        # Pass the input tensor 'x' through the projection_layer to perform the final classification.\n",
    "        # This maps the contextual representations to the number of classes for the classification task.\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int = 512,\n",
    "                      d_model: int=512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    # Create embedding layers for the source and target vocabularies\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create positional encoding layers for the source and target sequences\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create a list to hold encoder blocks\n",
    "    encoder_blocks = []\n",
    "    # Create 'N' encoder blocks and append them to the list\n",
    "    for _ in range(N):\n",
    "        # Create the self-attention block and feed-forward block for the encoder\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        # Create an encoder block using the self-attention and feed-forward blocks\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        # Add the encoder block to the list\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create a list to hold decoder blocks\n",
    "    decoder_blocks = []\n",
    "    # Create 'N' decoder blocks and append them to the list\n",
    "    for _ in range(N):\n",
    "        # Create the self-attention block, cross-attention block, and feed-forward block for the decoder\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        # Create a decoder block using the self-attention, cross-attention, and feed-forward blocks\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        # Add the decoder block to the list\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # Create the encoder and decoder using the ModuleList of encoder and decoder blocks\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer to map the contextual representations to the target vocabulary size\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer using the encoder, decoder, input embeddings, positional encodings, and projection layer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the weights of the transformer using Xavier uniform initialization\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # Return the built transformer\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Step 1: Split dataset into train set and temporary set (test + validation)\n",
    "train_size = int(0.7 * len(X))\n",
    "temp_size = len(X) - train_size\n",
    "train_dataset, temp_dataset = random_split(dataset, [train_size, temp_size])\n",
    "\n",
    "# Step 2: Split the temporary set into test and validation sets (without shuffling)\n",
    "test_size = int(0.5 * len(temp_dataset))\n",
    "val_size = len(temp_dataset) - test_size\n",
    "test_dataset, val_dataset = random_split(temp_dataset, [test_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4533, 1134, 4533, 1134)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = time_series_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train\u001b[39m.\u001b[39mshape, X_test\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape, y_test\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.8589e-01, -7.4098e+05,  4.1842e+01,  ..., -3.7641e-01,\n",
       "          -4.2820e-01, -3.5978e-01],\n",
       "         [ 8.0471e-01,  5.2894e+06,  3.4310e+01,  ..., -4.3196e-01,\n",
       "          -3.7641e-01, -4.2820e-01],\n",
       "         [ 3.7654e-01,  7.1006e+06,  6.8476e+00,  ..., -4.5715e-01,\n",
       "          -4.3196e-01, -3.7641e-01],\n",
       "         ...,\n",
       "         [-3.1682e-01,  2.0341e+10,  6.0830e+01,  ...,  6.7134e-01,\n",
       "           2.9493e-01, -1.8934e-02],\n",
       "         [ 2.1426e-01,  2.0289e+10,  5.6835e+01,  ...,  6.7695e-01,\n",
       "           6.7134e-01,  2.9493e-01],\n",
       "         [-9.6058e-01,  2.0273e+10,  6.4006e+01,  ...,  4.8859e-01,\n",
       "           6.7695e-01,  6.7134e-01]]),\n",
       " tensor([[-7.5918e-01,  2.0298e+10,  9.5028e+01,  ...,  1.1911e-01,\n",
       "           4.8859e-01,  6.7695e-01],\n",
       "         [ 8.3546e-01,  2.0387e+10,  9.9562e+01,  ..., -3.3670e-02,\n",
       "           1.1911e-01,  4.8859e-01],\n",
       "         [-1.7363e-01,  2.0438e+10,  8.2730e+01,  ...,  5.8430e-02,\n",
       "          -3.3670e-02,  1.1911e-01],\n",
       "         ...,\n",
       "         [ 7.3064e-01,  2.0677e+10,  1.6945e+01,  ..., -2.4438e+00,\n",
       "          -1.6634e+00, -1.8067e+00],\n",
       "         [-7.5427e-02,  2.0667e+10,  1.2426e+01,  ..., -3.0307e+00,\n",
       "          -2.4438e+00, -1.6634e+00],\n",
       "         [ 8.3737e-01,  2.0671e+10,  1.8391e+01,  ..., -3.5683e+00,\n",
       "          -3.0307e+00, -2.4438e+00]]),\n",
       " tensor([0, 1, 1,  ..., 1, 1, 0]),\n",
       " tensor([0, 0, 0,  ..., 2, 2, 1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.LongTensor(y_train.values)\n",
    "y_test = torch.LongTensor(y_test.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2298\n",
       "2    1960\n",
       "0    1409\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred.round()).sum().item()\n",
    "    acc = correct / len(y_true) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassModel(\n",
       "  (linear_layer_stack): Sequential(\n",
       "    (0): Linear(in_features=55, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a model\n",
    "class MulticlassModel(nn.Module):\n",
    "    def __init__(self, num_features, output_features, hidden_units=256):\n",
    "        super().__init__()\n",
    "        # Define layers\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features)\n",
    "            )\n",
    "    # Define forward method\n",
    "    def forward(self, X):\n",
    "        return self.linear_layer_stack(X)\n",
    "\n",
    "# Instantiate model class & send to target device\n",
    "model_4 = MulticlassModel(num_features=55,\n",
    "                          output_features=3\n",
    "                          ).to(device)\n",
    "model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlr_scheduler\u001b[39;00m \u001b[39mimport\u001b[39;00m ReduceLROnPlateau\n\u001b[1;32m      3\u001b[0m \u001b[39m# Training Loop & Testing loop\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Set random seed\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Put data on device\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X_train, X_test \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mto(device), X_test\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Training Loop & Testing loop\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Put data on device\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_4.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.1, verbose=True)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_4.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_logits = model_4(X_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # logits -> probs -> labels\n",
    "\n",
    "    # Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    train_acc = accuracy(y_true=y_train,\n",
    "                         y_pred=y_pred)\n",
    "    \n",
    "    optimizer.zero_grad() # reset gradients to zero\n",
    "    loss.backward() # backpropagation\n",
    "    optimizer.step() # gradient descent\n",
    "\n",
    "    model_4.eval()\n",
    "    with torch.no_grad():  # Use torch.no_grad() instead of torch.inference_mode()\n",
    "        \n",
    "        test_logits = model_4(X_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        # Update the learning rate based on the validation loss\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        test_acc = accuracy(y_true=y_test,\n",
    "                            y_pred=test_pred)\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Loss: {loss:.5f}, Train Acc: {train_acc:.2f}%, Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import talipp as tp\n",
    "from talipp.ohlcv import OHLCV\n",
    "from talipp.indicators import BOP, SOBV, Stoch, ForceIndex, MACD\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('etf_data.db')\n",
    "etf_list = ['spy', 'tlt', 'hyg', 'lqd', 'vnq']\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {etf_list[0]}\", connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={df.columns[4]: 'close'}, inplace=True)\n",
    "df = df.sort_values(by='date', ascending=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = [float(row['close']) for index,row in df.iterrows()]\n",
    "ohlcv = [OHLCV(float(row['open']), float(row['high']), float(row['low']), float(row['close']), float(row['volume']))\n",
    "         for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the indicators\n",
    "bop = BOP(ohlcv)\n",
    "sobv = SOBV(7, ohlcv)\n",
    "stoch = Stoch(14, 3, ohlcv)\n",
    "force_index = ForceIndex(13, ohlcv)\n",
    "macd = MACD(12, 26, 9, close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns in the DataFrame\n",
    "for i in range(5685):  # Assuming the length of the indicators matches the DataFrame length (5865 rows)\n",
    "    df.loc[i, 'BOP'] = bop[i]\n",
    "    df.loc[i, 'SOBV'] = sobv[i]\n",
    "    df.loc[i, 'Stoch'] = stoch[i].k\n",
    "    df.loc[i, 'ForceIndex'] = force_index[i]\n",
    "    df.loc[i, 'MACD'] = macd[i].histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['percentage_change'] = df['close'].shift(-5) / df['close'] - 1\n",
    "df['percentage_change'] = df['percentage_change'].shift(5)\n",
    "\n",
    "conditions = [\n",
    "    (df['percentage_change'] >= 0.01),\n",
    "    (df['percentage_change'] <= -0.01)\n",
    "]\n",
    "values = [2, 0]\n",
    "df['label'] = np.select(conditions, values, default=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'open', 'high', 'low', 'close', 'daily_change', 'perct_chg',\n",
       "       'volume', 'BOP', 'SOBV', 'Stoch', 'ForceIndex', 'MACD',\n",
       "       'percentage_change', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                   0\n",
       "open                   0\n",
       "high                   0\n",
       "low                    0\n",
       "close                  0\n",
       "daily_change           0\n",
       "perct_chg              0\n",
       "volume                 0\n",
       "BOP                  179\n",
       "SOBV                 179\n",
       "Stoch                179\n",
       "ForceIndex           179\n",
       "MACD                 187\n",
       "percentage_change      5\n",
       "label                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame\n",
    "# pd.set_option('display.max_rows', 20)  # Set to None to display all rows\n",
    "# pd.set_option('display.max_columns', None)  # Set to None to display all columns\n",
    "\n",
    "nan_counts = df.isna().sum()\n",
    "nan_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for a attention transformer model\n",
    "features = ['BOP', 'SOBV', 'Stoch', 'ForceIndex', 'MACD']\n",
    "\n",
    "# Choose the target column\n",
    "target = 'label'\n",
    "\n",
    "# Create X and y\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4541, 5), (1136, 5), (4541,), (1136,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# len(X_train), len(X_test), len(y_train), len(y_test)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4541, 5]),\n",
       " torch.Size([1136, 5]),\n",
       " torch.Size([4541, 1]),\n",
       " torch.Size([1136, 1]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.LongTensor(y_train.values.reshape(-1, 1))  # Reshape to [batch_size, 1]\n",
    "y_test = torch.LongTensor(y_test.values.reshape(-1, 1))  # Reshape to [batch_size, 1]\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs - raw data (words)\n",
    "\n",
    "embedding - vector of floats - (d_model)\n",
    "\n",
    "positional encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe shape: torch.Size([5, 128])\n",
      "position shape: torch.Size([5, 1])\n",
      "div_term shape: torch.Size([64])\n",
      "pe shape: torch.Size([5, 128])\n",
      "pe shape: torch.Size([5, 128])\n",
      "pe shape: torch.Size([1, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_len, d_model = 5, 128\n",
    "\n",
    "# Create a matrix of shape (seq_len, d_model) to hold the positional encodings.\n",
    "pe = torch.zeros(seq_len, d_model)\n",
    "print(f\"pe shape: {pe.shape}\")\n",
    "\n",
    "# Create a vector of shape (seq_len, 1) containing values from 0 to (seq_len - 1).\n",
    "position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # numerator\n",
    "print(f\"position shape: {position.shape}\")\n",
    "\n",
    "# Create a vector of shape (d_model, 1) containing values corresponding to the exponent for each dimension.\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # denominator in log space (more numerically stable)\n",
    "print(f\"div_term shape: {div_term.shape}\")\n",
    "\n",
    "# Apply sine to even indices of the matrix to get the positional encoding for the even dimensions.\n",
    "pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "print(f\"pe shape: {pe.shape}\")\n",
    "\n",
    "# Apply cosine to odd indices of the matrix to get the positional encoding for the odd dimensions.\n",
    "pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "print(f\"pe shape: {pe.shape}\")\n",
    "\n",
    "# Add a batch dimension to the positional encoding to make it compatible with batched input.\n",
    "pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "print(f\"pe shape: {pe.shape}\")\n",
    "\n",
    "# pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 237\u001b[0m\n\u001b[1;32m    234\u001b[0m trg_vocab_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m    235\u001b[0m model \u001b[39m=\u001b[39m Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 237\u001b[0m out \u001b[39m=\u001b[39m model(x, trg[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    238\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[81], line 221\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m    219\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_trg_mask(trg)\n\u001b[1;32m    220\u001b[0m enc_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, src_mask)\n\u001b[0;32m--> 221\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(trg_mask, enc_src, src_mask, trg_mask)\n\u001b[1;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[81], line 155\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, enc_out, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, enc_out, src_mask, trg_mask): \u001b[39m# x is the input to decoder\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     N, seq_length \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    156\u001b[0m     positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, seq_length)\u001b[39m.\u001b[39mexpand(N, seq_length)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    157\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embedding(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding(positions)))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "embeddings\n",
    "skip postional encodings not reqd\n",
    "layer vs batch normalization\n",
    "\n",
    "we need batch normalization (gamma & beta)\n",
    "- faster convergence\n",
    "- reduced sensitivity to initialization\n",
    "- regularization effect\n",
    "- higher learning rates\n",
    "\n",
    "\n",
    "\n",
    "Decoder\n",
    "Masked Multi head attention\n",
    "- makes the model causal. i.e. it can only attend to the previous tokens in the sequence\n",
    "- do that by making the dot product matrix negative infinity for the tokens that come after the current token (above the principal diagonal)\n",
    "- we do this before applying the softmax function as the softmax function will take care of making it 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A from scratch implementation of Transformer network,\n",
    "following the paper Attention is all you need with a\n",
    "few minor differences. I tried to make it as clear as\n",
    "possible to understand and also went through the code\n",
    "on my youtube channel!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
    "        queries = self.queries(query)  # (N, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "        device\n",
    "    )\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "        device\n",
    "    )\n",
    "    out = model(x, trg[:, :-1])\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the Attention Transformer model\n",
    "class AttentionTransformer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=16, num_layers=1, num_heads=4, dropout=0.1):\n",
    "        super(AttentionTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float))\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define the PositionalEncoding class for adding positional encodings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:x.size(0), :]\n",
    "\n",
    "# Create TensorDatasets for training and testing data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Set batch size and create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Instantiate the AttentionTransformer model\n",
    "input_size = X.shape[1]  # Number of features in X\n",
    "output_size = 3  # Number of unique classes in y\n",
    "model = AttentionTransformer(input_size=input_size, output_size=output_size)\n",
    "\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs = model(batch_X)\n",
    "\n",
    "outputs.shape\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(batch_X)\n",
    "\n",
    "#         outputs.shape\n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# # Evaluation on the test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "#     for batch_X, batch_y in test_loader:\n",
    "#         outputs = model(batch_X)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total_correct += (predicted == batch_y).sum().item()\n",
    "#         total_samples += batch_y.size(0)\n",
    "\n",
    "#     accuracy = total_correct / total_samples * 100\n",
    "#     print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training Loop & Testing loop\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Put data on device\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_transformer.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.1, verbose=True)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_transformer.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_logits = model_transformer(X_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # logits -> probs -> labels\n",
    "\n",
    "    # Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    train_acc = accuracy(y_true=y_train,\n",
    "                         y_pred=y_pred)\n",
    "    \n",
    "    optimizer.zero_grad() # reset gradients to zero\n",
    "    loss.backward() # backpropagation\n",
    "    optimizer.step() # gradient descent\n",
    "\n",
    "    model_transformer.eval()\n",
    "    with torch.no_grad():  # Use torch.no_grad() instead of torch.inference_mode()\n",
    "        \n",
    "        test_logits = model_transformer(X_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        # Update the learning rate based on the validation loss\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        test_acc = accuracy(y_true=y_test,\n",
    "                            y_pred=test_pred)\n",
    "        \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Loss: {loss:.5f}, Train Acc: {train_acc:.2f}%, Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
